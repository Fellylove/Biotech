{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": " copy of LS_DS_Unit_4_Sprint_Challenge_2_AG.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fellylove/Biotech/blob/master/copy_of_LS_DS_Unit_4_Sprint_Challenge_2_AG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOnp_5rgDP-7"
      },
      "source": [
        "\n",
        "## Autograded Notebook (Canvas & CodeGrade)\n",
        "\n",
        "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
        "Instructions\n",
        "\n",
        "- **Download** this notebook as you would any other ipynb file \n",
        "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
        "- **Delete** `raise NotImplementedError()`\n",
        "\n",
        "- **Write** your code in the `# YOUR CODE HERE` space\n",
        "\n",
        "\n",
        "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
        "\n",
        "- **Save** your notebook when you are finished\n",
        "- **Download** as a ipynb file (if working in Colab)\n",
        "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCvVmInPDP-9"
      },
      "source": [
        "\n",
        "## *Data Science Unit 4 Sprint 2*\n",
        "\n",
        "# Sprint Challenge - Neural Network Foundations\n",
        "\n",
        "Table of Problems\n",
        "\n",
        "1. [Defining Neural Networks](#Q1)\n",
        "2. [Simple Perceptron](#Q2)\n",
        "    - Perceptron\n",
        "    - Multilayer Perceptron (i.e. Neural Network)\n",
        "    - Analyze and Compare\n",
        "4. [Keras MMP](#Q3)\n",
        "\n",
        "\n",
        "____\n",
        "\n",
        "# Before you submit your notebook you must first\n",
        "\n",
        "1) Restart your notebook's Kernel\n",
        "\n",
        "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
        "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a2d017ba3200be3890c0b67eda283c48",
          "grade": false,
          "grade_id": "cell-621a8b86bacf295a",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "vnoLyMUEDP--"
      },
      "source": [
        "<a id=\"Q1\"></a>\n",
        "## 1. Defining Neural Networks \n",
        "\n",
        "Write *your own* definitions for the following terms:\n",
        "\n",
        "- **Neuron:** A neural is a container of a mathematical function which forms part of a network that is anologous to the biological neuron.\n",
        "\n",
        "- **Input Layer:** The input layer in a neural network recieves the input information which is then processed through mathematical activation functions. \n",
        "\n",
        "- **Hidden Layer:** Hidden layers either recieve input from the input layer or from a previous layer. Each hidden layer has a mathematical function that processes the information.\n",
        "\n",
        "- **Output Layer:** The outlayer receives input from previous layers process the informations through a mathematical functiona and outputs results. \n",
        "\n",
        "- **Activation:** This the process of transforming weighted sum of an input into an output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7233c31461609b21a7fc2651afb12632",
          "grade": true,
          "grade_id": "cell-6adae65226f09553",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "y_TnF-aYDP--"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10aa095d3db59bfe47f9823cfd62f1ef",
          "grade": false,
          "grade_id": "cell-d64f1de9e9458dc7",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "JoXmqUjsDP-_"
      },
      "source": [
        "- `Explain` how Back-propagation works \n",
        "\n",
        "- `Explain` how Gradient Descent works (mention the learning rate)\n",
        "\n",
        "\n",
        "- `Explain` how Back-propagation and Gradient Descent are related   \n",
        "\n",
        "Use your own words, but feel free to reference external materials for this question. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ceb3f64a4b1b18346decf75c8f5567d2",
          "grade": true,
          "grade_id": "cell-cef20b23d4e0b056",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ak9-fiHcDP_A"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "\n",
        "1.   Back-propagation computes the gradients of the loss function with respect to neural network parameters. It computes the gradients at the output layers and uses those gradients to compute the gradient at the previous layer and so on. And thus the weights are updated in reverse order at the end of each training epoch.\n",
        "\n",
        "2. Gradient Descent is an optimal algorithm that minimizes the cost function or error by finding the local-global minimum of the function. The learning rate determines how quickly the model is updated when finding the minimum error path. \n",
        "\n",
        "3. Back-propagation monitors the error loss made during Gradient Descent. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e040f3ddce6eb34b017f0eb685b202e6",
          "grade": false,
          "grade_id": "cell-e013d19857352d79",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "oBmlKDgdDP_A"
      },
      "source": [
        "Remember our Simple Perceptron Class from Monday. \n",
        "\n",
        "- Describe the process of making a prediction, i.e. how do you go from inputs to outputs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d746de6391012340f8548821850a621c",
          "grade": true,
          "grade_id": "cell-53c7cc36db9d7983",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MMhxIGr0DP_A"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "Neural networks make predictions using the the forward feedback process. The goal of the feedback network is to approxiamte a certain mathematical function that maps an input to output and thus make a prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQeXbS5DP_B"
      },
      "source": [
        "<a id=\"Q2\"></a>\n",
        "## 2. Simple Perceptron\n",
        "\n",
        "In this question, you will build two neural networks using `Keras`. After you build these two models, compare the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwaVbuMHDP_B"
      },
      "source": [
        "\"\"\"\n",
        "Our Dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\"Use this X & y in the following 2 models\"\n",
        "X = rng.randn(300, 2)\n",
        "y = np.array(np.logical_xor(X[:, 0] > 0, X[:, 1] > 0), \n",
        "             dtype=int)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aVTqVE3DP_B",
        "outputId": "82174c7a-b2bb-4f44-c006-ab1b01e4b598"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkF3VDaTDP_B",
        "outputId": "8723a3d2-0eab-47aa-8661-1d929b9a8398"
      },
      "source": [
        "y"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djHwQW3NDP_C",
        "outputId": "2c97008b-dfd2-4289-b958-672edfade5d0"
      },
      "source": [
        "2**2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O-C9PRqDP_C",
        "outputId": "654894e7-556d-49ee-c488-44afa4eac10f"
      },
      "source": [
        "4**4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmVhqB6DP_C",
        "outputId": "5cd4822b-a2bb-4df6-b95f-e58c2199f7b9"
      },
      "source": [
        "imput_dim = X[0].shape[0]\n",
        "imput_dim"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuzwQDTIDP_C"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78g31dFWDP_C"
      },
      "source": [
        "This word is speled wrong. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89LlghjFDP_C"
      },
      "source": [
        "### Simple Perceptron\n",
        "Construct a simple perceptron using Keras. \n",
        "\n",
        "Make sure to include the following in your model:\n",
        "- Add `1 dense layer` with a `single neuron` \n",
        "- Use a `sigmoid activation function`\n",
        "- Set `epochs` to 10 \n",
        "- Use the version of `crossentropy loss` that is appropriate for this data.\n",
        "\n",
        "Your model should be called `model1` and make sure to save the results of your fit statement to a variable called `h1`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67e9f7297eb22a79437494d713d74b71",
          "grade": false,
          "grade_id": "cell-427690628f9c900b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE_TX4mpDP_D",
        "outputId": "0bcee61c-1a00-4e5c-bbea-372ff7c9fb9c"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# build and fit model\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Dense(1, input_dim= imput_dim, activation='sigmoid'))\n",
        "model1.compile(loss='binary_crossentropy' ,\n",
        "              optimizer='sgd' ,\n",
        "              metrics=['accuracy'])\n",
        "epochs = 10\n",
        "\n",
        "h1 = model1.fit(X, y, \n",
        "                    epochs=epochs, \n",
        "                    validation_split=0.2\n",
        "                   )\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.7811 - accuracy: 0.4208 - val_loss: 0.7225 - val_accuracy: 0.4833\n",
            "Epoch 2/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7777 - accuracy: 0.4167 - val_loss: 0.7206 - val_accuracy: 0.4833\n",
            "Epoch 3/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7742 - accuracy: 0.4208 - val_loss: 0.7189 - val_accuracy: 0.4833\n",
            "Epoch 4/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7709 - accuracy: 0.4208 - val_loss: 0.7173 - val_accuracy: 0.4833\n",
            "Epoch 5/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7677 - accuracy: 0.4208 - val_loss: 0.7158 - val_accuracy: 0.4833\n",
            "Epoch 6/10\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7645 - accuracy: 0.4208 - val_loss: 0.7143 - val_accuracy: 0.4833\n",
            "Epoch 7/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7616 - accuracy: 0.4167 - val_loss: 0.7128 - val_accuracy: 0.4833\n",
            "Epoch 8/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7586 - accuracy: 0.4250 - val_loss: 0.7116 - val_accuracy: 0.4833\n",
            "Epoch 9/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7558 - accuracy: 0.4208 - val_loss: 0.7102 - val_accuracy: 0.4833\n",
            "Epoch 10/10\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7528 - accuracy: 0.4208 - val_loss: 0.7090 - val_accuracy: 0.4833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36f7f830036d0443ca8e8ba0f17b2a4e",
          "grade": true,
          "grade_id": "cell-bf2ae566afacde8c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eQJoXGW7DP_D"
      },
      "source": [
        "# Visible test\n",
        "assert len(model1.get_config()[\"layers\"]) == 2, \"Make sure you only create 1 Dense layer.\"\n",
        "assert len(h1.epoch) <=10, \"Did you make sure to set epochs to 10 or less?\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkxsw9a5DP_D",
        "outputId": "9c8ee8de-59eb-4831-af07-509ba234e7ef"
      },
      "source": [
        "model1.get_config()[\"layers\"]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'class_name': 'InputLayer',\n",
              "  'config': {'batch_input_shape': (None, 2),\n",
              "   'dtype': 'float32',\n",
              "   'name': 'dense_input',\n",
              "   'ragged': False,\n",
              "   'sparse': False}},\n",
              " {'class_name': 'Dense',\n",
              "  'config': {'activation': 'sigmoid',\n",
              "   'activity_regularizer': None,\n",
              "   'batch_input_shape': (None, 2),\n",
              "   'bias_constraint': None,\n",
              "   'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "   'bias_regularizer': None,\n",
              "   'dtype': 'float32',\n",
              "   'kernel_constraint': None,\n",
              "   'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "    'config': {'seed': None}},\n",
              "   'kernel_regularizer': None,\n",
              "   'name': 'dense',\n",
              "   'trainable': True,\n",
              "   'units': 1,\n",
              "   'use_bias': True}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "95d3ee2935a0de64f2a5a22460520e69",
          "grade": true,
          "grade_id": "cell-a957e14380b2f508",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_JEQ_P-fDP_D"
      },
      "source": [
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGMvg0KeDP_D"
      },
      "source": [
        "### Multi-Layer Perceptron\n",
        "Now construct a multi-layer perceptron model (also known as a neural network). \n",
        "\n",
        "Your neural network `must` have: \n",
        "- `2` Hidden Layers\n",
        "- Select any number between `5-32` for the number of neurons in each hidden layers\n",
        "- Your pick of activation function and optimizer\n",
        "- Incorporate the `Callback function` below into your model\n",
        "- Set epochs to `100`\n",
        "- Your model should be called `model2` \n",
        "- Save the results of your fit statement to a variable called `h2`. \n",
        "- Use the version of `crossentropy loss` that is appropriate for this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgUs0uFWDP_E"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback): \n",
        "    def on_epoch_end(self, epoch, logs={}): \n",
        "        # if model reaches 99% accuracy, training is terminated \n",
        "        acc_threshold = 0.99\n",
        "        if(logs.get('accuracy') > acc_threshold):   \n",
        "            self.model.stop_training = True\n",
        "            self.model.callback_used = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "314337f29c8cd7f38224a31687a86b12",
          "grade": false,
          "grade_id": "cell-77523c4c64743f16",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsmPlNw7DP_E",
        "outputId": "5100f7d1-cff5-4ec5-a6f8-415de34ae644"
      },
      "source": [
        "# build and fit model\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Dense(10, input_dim= imput_dim, activation='relu'))\n",
        "model2.add(Dense(5, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss='binary_crossentropy' ,\n",
        "              optimizer='sgd' ,\n",
        "              metrics=['accuracy'])\n",
        "epochs = 100\n",
        "\n",
        "h2 = model2.fit(X, y, \n",
        "                    epochs=epochs, \n",
        "                    validation_split=0.2,\n",
        "                callbacks = [myCallback()]\n",
        "                   )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 0.6864 - accuracy: 0.4333 - val_loss: 0.6789 - val_accuracy: 0.4500\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6851 - accuracy: 0.4375 - val_loss: 0.6781 - val_accuracy: 0.4500\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6837 - accuracy: 0.4375 - val_loss: 0.6772 - val_accuracy: 0.4500\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6823 - accuracy: 0.4500 - val_loss: 0.6765 - val_accuracy: 0.4500\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.4458 - val_loss: 0.6758 - val_accuracy: 0.4667\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6799 - accuracy: 0.4500 - val_loss: 0.6751 - val_accuracy: 0.4667\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6787 - accuracy: 0.4542 - val_loss: 0.6744 - val_accuracy: 0.4500\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6775 - accuracy: 0.4542 - val_loss: 0.6737 - val_accuracy: 0.4500\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.4667 - val_loss: 0.6729 - val_accuracy: 0.4667\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6748 - accuracy: 0.4958 - val_loss: 0.6721 - val_accuracy: 0.4333\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6734 - accuracy: 0.5458 - val_loss: 0.6714 - val_accuracy: 0.4833\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6720 - accuracy: 0.6250 - val_loss: 0.6705 - val_accuracy: 0.5333\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6707 - accuracy: 0.6333 - val_loss: 0.6698 - val_accuracy: 0.5167\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.6333 - val_loss: 0.6690 - val_accuracy: 0.5167\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6679 - accuracy: 0.6333 - val_loss: 0.6682 - val_accuracy: 0.5333\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6662 - accuracy: 0.6375 - val_loss: 0.6673 - val_accuracy: 0.5333\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6647 - accuracy: 0.6250 - val_loss: 0.6664 - val_accuracy: 0.5333\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.6125 - val_loss: 0.6654 - val_accuracy: 0.5333\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6167 - val_loss: 0.6646 - val_accuracy: 0.5333\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6598 - accuracy: 0.6125 - val_loss: 0.6637 - val_accuracy: 0.5500\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6582 - accuracy: 0.6125 - val_loss: 0.6628 - val_accuracy: 0.5500\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6567 - accuracy: 0.6125 - val_loss: 0.6619 - val_accuracy: 0.5333\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6552 - accuracy: 0.6167 - val_loss: 0.6609 - val_accuracy: 0.5500\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6536 - accuracy: 0.6125 - val_loss: 0.6599 - val_accuracy: 0.5500\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6523 - accuracy: 0.6167 - val_loss: 0.6589 - val_accuracy: 0.5500\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6507 - accuracy: 0.6292 - val_loss: 0.6579 - val_accuracy: 0.5500\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6492 - accuracy: 0.6333 - val_loss: 0.6569 - val_accuracy: 0.5500\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6477 - accuracy: 0.6333 - val_loss: 0.6559 - val_accuracy: 0.5333\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6461 - accuracy: 0.6292 - val_loss: 0.6549 - val_accuracy: 0.5333\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6446 - accuracy: 0.6333 - val_loss: 0.6538 - val_accuracy: 0.5333\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6430 - accuracy: 0.6292 - val_loss: 0.6528 - val_accuracy: 0.5333\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.6333 - val_loss: 0.6517 - val_accuracy: 0.5500\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6400 - accuracy: 0.6500 - val_loss: 0.6507 - val_accuracy: 0.5500\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6385 - accuracy: 0.6583 - val_loss: 0.6496 - val_accuracy: 0.6000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6371 - accuracy: 0.6667 - val_loss: 0.6485 - val_accuracy: 0.6333\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.6750 - val_loss: 0.6477 - val_accuracy: 0.6333\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6342 - accuracy: 0.6750 - val_loss: 0.6468 - val_accuracy: 0.6167\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6327 - accuracy: 0.6833 - val_loss: 0.6458 - val_accuracy: 0.6333\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6312 - accuracy: 0.6875 - val_loss: 0.6447 - val_accuracy: 0.6500\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6298 - accuracy: 0.6958 - val_loss: 0.6437 - val_accuracy: 0.6667\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6284 - accuracy: 0.7000 - val_loss: 0.6426 - val_accuracy: 0.6833\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6270 - accuracy: 0.7000 - val_loss: 0.6416 - val_accuracy: 0.6833\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6255 - accuracy: 0.7000 - val_loss: 0.6405 - val_accuracy: 0.6833\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6241 - accuracy: 0.7042 - val_loss: 0.6392 - val_accuracy: 0.7000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6226 - accuracy: 0.7042 - val_loss: 0.6382 - val_accuracy: 0.7000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6210 - accuracy: 0.7167 - val_loss: 0.6370 - val_accuracy: 0.7000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6195 - accuracy: 0.7250 - val_loss: 0.6358 - val_accuracy: 0.7000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6179 - accuracy: 0.7250 - val_loss: 0.6346 - val_accuracy: 0.7000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6164 - accuracy: 0.7292 - val_loss: 0.6335 - val_accuracy: 0.7333\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6149 - accuracy: 0.7292 - val_loss: 0.6324 - val_accuracy: 0.7333\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6132 - accuracy: 0.7375 - val_loss: 0.6311 - val_accuracy: 0.7333\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6116 - accuracy: 0.7375 - val_loss: 0.6299 - val_accuracy: 0.7333\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6099 - accuracy: 0.7333 - val_loss: 0.6286 - val_accuracy: 0.7333\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6083 - accuracy: 0.7417 - val_loss: 0.6270 - val_accuracy: 0.7333\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6065 - accuracy: 0.7417 - val_loss: 0.6257 - val_accuracy: 0.7333\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6048 - accuracy: 0.7458 - val_loss: 0.6245 - val_accuracy: 0.7333\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6029 - accuracy: 0.7458 - val_loss: 0.6230 - val_accuracy: 0.7167\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6011 - accuracy: 0.7458 - val_loss: 0.6216 - val_accuracy: 0.7333\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5993 - accuracy: 0.7458 - val_loss: 0.6200 - val_accuracy: 0.7167\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5974 - accuracy: 0.7458 - val_loss: 0.6184 - val_accuracy: 0.7333\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5953 - accuracy: 0.7500 - val_loss: 0.6170 - val_accuracy: 0.7333\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5934 - accuracy: 0.7542 - val_loss: 0.6154 - val_accuracy: 0.7333\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5914 - accuracy: 0.7583 - val_loss: 0.6139 - val_accuracy: 0.7500\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5894 - accuracy: 0.7625 - val_loss: 0.6124 - val_accuracy: 0.7500\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5874 - accuracy: 0.7583 - val_loss: 0.6109 - val_accuracy: 0.7500\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5852 - accuracy: 0.7667 - val_loss: 0.6090 - val_accuracy: 0.7500\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5831 - accuracy: 0.7708 - val_loss: 0.6074 - val_accuracy: 0.7667\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5809 - accuracy: 0.7750 - val_loss: 0.6057 - val_accuracy: 0.7667\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5788 - accuracy: 0.7792 - val_loss: 0.6040 - val_accuracy: 0.7833\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5766 - accuracy: 0.7917 - val_loss: 0.6023 - val_accuracy: 0.7833\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5743 - accuracy: 0.8000 - val_loss: 0.6004 - val_accuracy: 0.8000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5720 - accuracy: 0.8042 - val_loss: 0.5985 - val_accuracy: 0.7833\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5698 - accuracy: 0.8000 - val_loss: 0.5966 - val_accuracy: 0.8167\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5674 - accuracy: 0.8000 - val_loss: 0.5947 - val_accuracy: 0.8167\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5650 - accuracy: 0.8000 - val_loss: 0.5926 - val_accuracy: 0.8167\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5626 - accuracy: 0.8125 - val_loss: 0.5905 - val_accuracy: 0.8167\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.8167 - val_loss: 0.5888 - val_accuracy: 0.8167\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5577 - accuracy: 0.8250 - val_loss: 0.5866 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5552 - accuracy: 0.8250 - val_loss: 0.5845 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5527 - accuracy: 0.8292 - val_loss: 0.5824 - val_accuracy: 0.8333\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5501 - accuracy: 0.8375 - val_loss: 0.5802 - val_accuracy: 0.8333\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5475 - accuracy: 0.8375 - val_loss: 0.5780 - val_accuracy: 0.8333\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5449 - accuracy: 0.8375 - val_loss: 0.5757 - val_accuracy: 0.8333\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5422 - accuracy: 0.8458 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5395 - accuracy: 0.8458 - val_loss: 0.5712 - val_accuracy: 0.8333\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5369 - accuracy: 0.8542 - val_loss: 0.5691 - val_accuracy: 0.8333\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5340 - accuracy: 0.8542 - val_loss: 0.5668 - val_accuracy: 0.8500\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5313 - accuracy: 0.8542 - val_loss: 0.5647 - val_accuracy: 0.8500\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5285 - accuracy: 0.8667 - val_loss: 0.5623 - val_accuracy: 0.8500\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5258 - accuracy: 0.8667 - val_loss: 0.5599 - val_accuracy: 0.8500\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5230 - accuracy: 0.8667 - val_loss: 0.5575 - val_accuracy: 0.8500\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5202 - accuracy: 0.8750 - val_loss: 0.5551 - val_accuracy: 0.8500\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5175 - accuracy: 0.8708 - val_loss: 0.5525 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5147 - accuracy: 0.8708 - val_loss: 0.5502 - val_accuracy: 0.8500\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5120 - accuracy: 0.8708 - val_loss: 0.5477 - val_accuracy: 0.8500\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5093 - accuracy: 0.8708 - val_loss: 0.5449 - val_accuracy: 0.8500\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5064 - accuracy: 0.8750 - val_loss: 0.5422 - val_accuracy: 0.8500\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5035 - accuracy: 0.8750 - val_loss: 0.5393 - val_accuracy: 0.8500\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5006 - accuracy: 0.8792 - val_loss: 0.5368 - val_accuracy: 0.8500\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4977 - accuracy: 0.8833 - val_loss: 0.5340 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a5f575f46f151f97f1cebc19a484bae",
          "grade": true,
          "grade_id": "cell-770612ca24334d8a",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PUawdiTQDP_E"
      },
      "source": [
        "# Visible test\n",
        "assert len(model2.get_config()[\"layers\"]) == 4, \"You should have 4 layers: Input, hidden 1, hidden 2, output.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][1][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 1, but don't.\"\n",
        "assert 5 <= model2.get_config()[\"layers\"][2][\"config\"][\"units\"] <= 32, \"You should have 5 - 32 units in hidden layer 2, but don't.\"\n",
        "assert h2.params[\"epochs\"] == 100, \"You didn't set epochs to 100.\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3ca73d4d3d17897a570e19a8a97c050f",
          "grade": true,
          "grade_id": "cell-49b1bf7cce22b5b9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-VLL7GkCDP_E"
      },
      "source": [
        "# Hidden tests - you will see the results when you submit to Canvas"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrfS4WByDP_E"
      },
      "source": [
        "### Analyze and Compare\n",
        "\n",
        "**Before you Start**: You will need to install an additional library for this next segment. \n",
        "\n",
        "Install the package `mlxtend` into the environment you are using for the sprint challenge.\n",
        "\n",
        "You can install this package using the following statement in the terminal\n",
        "\n",
        "```python\n",
        "pip install mlxtend\n",
        "```\n",
        "\n",
        "Or you can install this package using the following statement in your notebook\n",
        "\n",
        "```python\n",
        "!pip install mlxtend\n",
        "```\n",
        "\n",
        "If you choose to install this package from within your notebook, be sure to delete the install statement afterwards so that CodeGrade doesn't try to install it and potentially crash. \n",
        "\n",
        "\n",
        "The cells below generate decision boundary plots of your models (`model1` & `model2`). Review the plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGauHrjlMcQX",
        "outputId": "05a95e56-79a2-490d-e661-cec544dd5755"
      },
      "source": [
        "!pip install mlxtend"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (56.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyOYqr1sDP_F",
        "outputId": "dbbafd3e-b1e6-4732-cdbf-14d4afd8c30f"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((300, 2), (300,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "LpbQKKgaDP_F",
        "outputId": "a588c7ea-dce4-448b-932a-c0b4df33d474"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "for clf, hist, name, grd in zip([model1,model2], [h1, h2],['Perceptron', 'Multi-Layer Perceptron'],[1,2]):\n",
        "\n",
        "    ax = plt.subplot(1,2, grd)\n",
        "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
        "    title = f\"{name} with {hist.history['accuracy'][-1]:,.2f} Accuracy\"\n",
        "    plt.title(title)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mlxtend/plotting/decision_regions.py:244: MatplotlibDeprecationWarning: Passing unsupported keyword arguments to axis() will raise a TypeError in 3.3.\n",
            "  ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "40c30e638b6defe125180b9832a675e2",
          "grade": false,
          "grade_id": "cell-b1bde9222e35b3fc",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": true
        },
        "id": "O3rjMgfXDP_F"
      },
      "source": [
        "1) Why does the Perceptron (`model1`) only achieve about 50% accuracy? \n",
        "\n",
        "2) What is the architectural property of the Multi-Layer Perceptron that allows it to more accurately learn the relationship between X and y? \n",
        "- Hint: recall that each layer represents a vector space and they usually have a different number of dimensions, $\\mathbb{R}^N$.\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e01b50ff508342b905c5a4cdbd7d2dc4",
          "grade": true,
          "grade_id": "cell-302694c508c8da0e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xdQhmXZdDP_G"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "1. The Perceptron model1 fails to achieve a higher accuracy because the single peceptron creates a simple neural network that doesn't have the complexity to learn the pattern between X and Y. It uses the linear function to solve the problem.\n",
        "2. Multi-layer perceptron accurately learns the relationship between X and y becuase of its additional layers that can separate the data patterns with multiple lines that form different shapes. It solves the problem by incorporating non-linear functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYL-wZ0uDP_G"
      },
      "source": [
        "## 3. Keras MMP <a id=\"Q3\"></a>\n",
        "\n",
        "- Implement a Multilayer Perceptron architecture of your choosing using the Keras library. \n",
        "- Train your model and report its baseline accuracy. \n",
        "- Then `hyper-parameters tune two parameters each with no more than 2 values each`\n",
        "    - Due to limited computational resources on CodeGrade `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE`\n",
        "- Report your optimized model's accuracy\n",
        "- Use the Heart Disease Dataset provided (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network.\n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyper-parameters tune your model. \n",
        "    - **Use `n_jobs` = 1**\n",
        "- When hyper-parameters tuning, show you work by adding code cells for each new experiment.\n",
        "- Report the accuracy for each combination of hyper-parameters as you test them so that we can easily see which resulted in the highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "inputHidden": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "outputHidden": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "3bq8luNiDP_H",
        "outputId": "e9c64dde-6924-4068-9d76-559e23ae01d8"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load data\n",
        "data_path = 'https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df = df.sample(frac=1)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(303, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>275</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>139</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>44</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>112</td>\n",
              "      <td>290</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>153</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>110</td>\n",
              "      <td>265</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>222</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>142</td>\n",
              "      <td>309</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
              "11    48    0   2       130   275    0  ...      0      0.2      2   0     2       1\n",
              "185   44    1   0       112   290    0  ...      0      0.0      2   1     2       0\n",
              "60    71    0   2       110   265    1  ...      0      0.0      2   1     2       1\n",
              "56    48    1   0       122   222    0  ...      0      0.0      2   0     2       1\n",
              "255   45    1   0       142   309    0  ...      1      0.0      1   3     3       0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "22de1dc5d17d7a0bc674d082c33e8b65",
          "grade": false,
          "grade_id": "cell-85dc40f19f5a1d6b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "rFE4VeR_DP_H"
      },
      "source": [
        "# Create an input matrix named 'X' store it in a 2D numpy array\n",
        "\n",
        "# Create an output vector for the labels named 'Y', store it in 1D numpy array\n",
        "\n",
        "\n",
        "\n",
        "df1 = df[['age',\t'sex',\t'cp',\t'trestbps',\t'chol',\t'fbs',\t'restecg','thalach',\t'exang',\t'oldpeak',\t'slope',\t'ca',\t'thal']]\n",
        "X = df1.to_numpy()\n",
        "\n",
        "Y = df['target'].to_numpy()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwbipzEmDP_H",
        "outputId": "9332084d-44ba-4717-da8b-818ac60da226"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(303, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "825d4f808810a2a8d6301d7453afe478",
          "grade": true,
          "grade_id": "cell-c17c686c974edc2e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "oOKHjFWpDP_H"
      },
      "source": [
        "# Visible Testing\n",
        "assert X.shape[0] == 303, \"Did you drop/lose some rows in X? Did you properly load and split the data?\"\n",
        "assert X.shape[1] == 13, \"Did you drop/lose some columns in X? Did you properly load and split the data?\"\n",
        "assert len(Y)== 303, \"Did you drop/lose some rows in Y? Did you properly load and split the data?\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBGJSf3mDP_I"
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvHyFVh0cKYK"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "a = StandardScaler()\n",
        "a.fit(X)\n",
        "X_standardized = a.transform(X)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "475835631ff6a34028443dbf604bd922",
          "grade": false,
          "grade_id": "cell-cfc5517cd0b6fa64",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Hkp2woM2DP_I"
      },
      "source": [
        "# Create a function named 'create_model' that returns a complied keras model -  required for KerasClassifier\n",
        "# YOUR CODE HERE\n",
        "def create_model(units=12):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units, activation='relu', input_dim=13))\n",
        "    model.add(Dense(6, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7b906697afb0a3b52cd19e9548eae6a7",
          "grade": true,
          "grade_id": "cell-fac25126eaf1eee4",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SPYFtparDP_I"
      },
      "source": [
        "# Visible Testing\n",
        "assert create_model().__module__ == 'tensorflow.python.keras.engine.sequential', \"create_model should return a keras model that was created using the Sequential class.\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0412c74b7803790452d4914d99995dd2",
          "grade": false,
          "grade_id": "cell-fbc3d0a07230078c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fa38gJMvDP_I"
      },
      "source": [
        "# Pass 'create_model' into KerasClassifier, store KerasClassifier to a variable named 'model'\n",
        "# YOUR CODE HERE\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0442c29a94065e922c5ae929976a52ab",
          "grade": true,
          "grade_id": "cell-464e7506993775f2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "h83tNvctDP_J"
      },
      "source": [
        "# Visible Testing\n",
        "assert model.__module__ == 'tensorflow.python.keras.wrappers.scikit_learn', \"model should be a instance of KerasClassifier.\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f88603ef37a4d3d2ef8699a41ac9a0b2",
          "grade": false,
          "grade_id": "cell-985c0425f3b1304d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "karzLe_BDP_J"
      },
      "source": [
        "# Define the grid search parameters inside a dictionary named 'param_grid' \n",
        "# Use 2 hyper-parameters with 2 possible values for each \n",
        "\n",
        "# YOUR CODE HERE\n",
        "param_grid = {\n",
        "    'units': [24, 36],\n",
        "    'epochs': [30, 40],\n",
        "}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a551fd8278b30c1318c036f6ad43b503",
          "grade": true,
          "grade_id": "cell-c765b5db5489d7a2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sFngxP1YDP_K"
      },
      "source": [
        "assert len(param_grid.keys()) == 2, \"Did you create a param dict with 2 hyper-parameters as keys?\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ea6312f4bc1f42809196b696037dd52",
          "grade": false,
          "grade_id": "cell-7cfb4315eab5031c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auKEMINQDP_K",
        "outputId": "822453ce-acad-426c-a2eb-a56591ef111a"
      },
      "source": [
        "# Create Grid Search object and name it 'gs'\n",
        "# Run Grid Search \n",
        "# YOUR CODE HERE\n",
        "gs = GridSearchCV(model, param_grid, n_jobs=1)\n",
        "grid_result = gs.fit(X_standardized, Y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - 1s 2ms/step - loss: 0.7250 - accuracy: 0.4793\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5413\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6544 - accuracy: 0.5826\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.6405\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.6818\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.7066\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7273\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.7438\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5429 - accuracy: 0.7603\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7686\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7851\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7893\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.7934\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.8058\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8058\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4594 - accuracy: 0.8140\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.8140\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.8182\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.8140\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8264\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8306\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8347\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8388\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8512\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8512\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8512\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8595\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8636\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3644 - accuracy: 0.8678\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8719\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3798 - accuracy: 0.8525\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6844 - accuracy: 0.4545\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6606 - accuracy: 0.5041\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.5579\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6177 - accuracy: 0.5785\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5976 - accuracy: 0.6694\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5796 - accuracy: 0.7149\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7397\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.7521\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7562\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7769\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7851\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7893\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7851\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7975\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4507 - accuracy: 0.7975\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8017\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.8140\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8182\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.8223\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8264\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8388\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3737 - accuracy: 0.8430\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8388\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8388\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3519 - accuracy: 0.8471\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8512\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8512\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8554\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8636\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8636\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3526 - accuracy: 0.8525\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6453 - accuracy: 0.5868\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.6405\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5826 - accuracy: 0.6901\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.7107\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7397\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.7521\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7727\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7893\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.8017\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8140\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8347\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8471\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.8512\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8512\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8595\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8636\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8719\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8760\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3616 - accuracy: 0.8802\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8760\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8760\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8760\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8760\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8760\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3332 - accuracy: 0.8760\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8760\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8760\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3225 - accuracy: 0.8719\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8760\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8719\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4259 - accuracy: 0.8033\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7559 - accuracy: 0.5679\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7080 - accuracy: 0.5761\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.6008\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.6420\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6109 - accuracy: 0.6914\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.7078\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5680 - accuracy: 0.7449\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5481 - accuracy: 0.7572\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7819\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7984\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8148\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.8148\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4614 - accuracy: 0.8272\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.8354\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8436\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8436\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8477\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8642\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8683\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8683\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3590 - accuracy: 0.8765\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3501 - accuracy: 0.8848\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8848\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8848\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8848\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8889\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3151 - accuracy: 0.8889\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8930\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8971\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.8971\n",
            "WARNING:tensorflow:5 out of the last 207 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7018cd5a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.5747 - accuracy: 0.7333\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7695 - accuracy: 0.5144\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.5514\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6731 - accuracy: 0.5802\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6354 - accuracy: 0.6173\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.6461\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5772 - accuracy: 0.6914\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7284\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7407\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7490\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7572\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7613\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7695\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7778\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.7819\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4489 - accuracy: 0.7860\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.7860\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.7942\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8025\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8025\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8107\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4069 - accuracy: 0.8189\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8230\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8313\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8313\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8395\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8436\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8436\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8519\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.8519\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8519\n",
            "WARNING:tensorflow:6 out of the last 209 calls to <function Model.make_test_function.<locals>.test_function at 0x7f7017b12cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3233 - accuracy: 0.8500\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7363 - accuracy: 0.4628\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.5702\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6375 - accuracy: 0.6653\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6029 - accuracy: 0.7149\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.7273\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5441 - accuracy: 0.7645\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7810\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.8017\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8099\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4448 - accuracy: 0.8264\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8306\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8306\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8347\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8471\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8430\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8512\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3570 - accuracy: 0.8512\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8554\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3423 - accuracy: 0.8595\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8595\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8595\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8595\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8595\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3151 - accuracy: 0.8595\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3111 - accuracy: 0.8636\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3068 - accuracy: 0.8678\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3032 - accuracy: 0.8719\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8719\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.8719\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2921 - accuracy: 0.8719\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.3900 - accuracy: 0.8852\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7240 - accuracy: 0.5165\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6737 - accuracy: 0.5950\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6386 - accuracy: 0.6901\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.7355\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7810\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5472 - accuracy: 0.8058\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5238 - accuracy: 0.8223\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.8388\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4745 - accuracy: 0.8306\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8306\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8347\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8430\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8430\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8471\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8471\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8430\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3589 - accuracy: 0.8512\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3504 - accuracy: 0.8554\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8595\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8678\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8678\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8719\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8719\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8719\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3073 - accuracy: 0.8760\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8760\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8843\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8884\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2905 - accuracy: 0.8884\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.8884\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3719 - accuracy: 0.8525\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5496\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6394 - accuracy: 0.6529\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.7438\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5578 - accuracy: 0.7769\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7934\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.8182\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8347\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8306\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4173 - accuracy: 0.8471\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3978 - accuracy: 0.8554\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8554\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3657 - accuracy: 0.8595\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8595\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8678\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8802\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8884\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3145 - accuracy: 0.8802\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8884\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8884\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2925 - accuracy: 0.8967\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.9008\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2807 - accuracy: 0.9050\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2750 - accuracy: 0.9091\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.9091\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.9091\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.9091\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2566 - accuracy: 0.9091\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2514 - accuracy: 0.9091\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9091\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2439 - accuracy: 0.9091\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4105 - accuracy: 0.8033\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7547 - accuracy: 0.3909\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7021 - accuracy: 0.5432\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.6173\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6406 - accuracy: 0.6914\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6177 - accuracy: 0.7572\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.7942\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.8148\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.8354\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.8395\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4914 - accuracy: 0.8354\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.8313\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8354\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8395\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.8395\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8436\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8395\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3645 - accuracy: 0.8436\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3535 - accuracy: 0.8436\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8477\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8560\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8601\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8642\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3150 - accuracy: 0.8683\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8683\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3049 - accuracy: 0.8683\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.8683\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8807\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8848\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.8848\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.8889\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5618 - accuracy: 0.7333\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7073 - accuracy: 0.5720\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6597 - accuracy: 0.6626\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6215 - accuracy: 0.6955\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5887 - accuracy: 0.7119\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.7325\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7449\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5090 - accuracy: 0.7737\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.7819\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7860\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.8066\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8107\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8148\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8107\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8107\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8148\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8189\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8230\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8230\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8272\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.8313\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3501 - accuracy: 0.8313\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8313\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.8395\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3376 - accuracy: 0.8395\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3334 - accuracy: 0.8436\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8519\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8560\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8601\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.8642\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8601\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2789 - accuracy: 0.8833\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7153 - accuracy: 0.5702\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.6074\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6405\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.6736\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5937 - accuracy: 0.7107\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5711 - accuracy: 0.7273\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.7314\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7438\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7521\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7603\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7769\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7893\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7934\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.7975\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.7934\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8099\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3940 - accuracy: 0.8182\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8388\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8471\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8471\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3611 - accuracy: 0.8471\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.8471\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8471\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8471\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8512\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8512\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8595\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8595\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8554\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3176 - accuracy: 0.8512\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.8512\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8554\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3084 - accuracy: 0.8678\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3058 - accuracy: 0.8719\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8719\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8719\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2979 - accuracy: 0.8760\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8802\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2928 - accuracy: 0.8760\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.8719\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3026 - accuracy: 0.8361\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6870 - accuracy: 0.5083\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6543 - accuracy: 0.5537\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6262 - accuracy: 0.6364\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.6860\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5763 - accuracy: 0.7273\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7603\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5279 - accuracy: 0.7934\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5062 - accuracy: 0.8058\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.8182\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.8223\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4467 - accuracy: 0.8182\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8306\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8306\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8430\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3895 - accuracy: 0.8512\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8595\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8554\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8554\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3529 - accuracy: 0.8554\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8554\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8595\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.8636\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8636\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3261 - accuracy: 0.8636\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8636\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8636\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8678\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8678\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8678\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8719\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8719\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8719\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8760\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8760\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8760\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8760\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.8802\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.8802\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.8802\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8843\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4112 - accuracy: 0.8689\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.7082 - accuracy: 0.5331\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.5950\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.6446\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.7066\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6147 - accuracy: 0.7521\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5951 - accuracy: 0.7810\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5760 - accuracy: 0.8017\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5568 - accuracy: 0.7975\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5369 - accuracy: 0.8140\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 0.8182\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.8264\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.8223\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.8223\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8306\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8264\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8347\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8388\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8388\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8471\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8471\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3614 - accuracy: 0.8430\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8430\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3489 - accuracy: 0.8430\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8471\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8554\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8636\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8678\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8678\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8719\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8760\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8760\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3102 - accuracy: 0.8802\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.8802\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3028 - accuracy: 0.8802\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8802\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8802\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8802\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8802\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.8802\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2851 - accuracy: 0.8802\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.3874 - accuracy: 0.8525\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5926\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.6667\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5918 - accuracy: 0.7366\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5605 - accuracy: 0.7654\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5318 - accuracy: 0.8025\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.8313\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4803 - accuracy: 0.8477\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.8601\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8477\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8683\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4027 - accuracy: 0.8683\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8724\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8765\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8848\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3508 - accuracy: 0.8848\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8848\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8848\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8848\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3151 - accuracy: 0.8848\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8889\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8889\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2961 - accuracy: 0.8930\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8971\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.8971\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2810 - accuracy: 0.8971\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.8971\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8971\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.9012\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.8971\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2618 - accuracy: 0.9012\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8971\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.8930\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2532 - accuracy: 0.8930\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2501 - accuracy: 0.8930\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.8930\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9012\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.8971\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.8971\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.8971\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2353 - accuracy: 0.8971\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6998 - accuracy: 0.7000\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5637 - accuracy: 0.7366\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.7325\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7449\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7572\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7695\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.7819\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.7984\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.8025\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8066\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8066\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8148\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8148\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8066\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8107\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8148\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8272\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3641 - accuracy: 0.8313\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8354\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8354\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3486 - accuracy: 0.8354\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8436\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8395\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8395\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3328 - accuracy: 0.8436\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8477\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8477\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8560\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8642\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8683\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.8601\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8642\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8642\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8683\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8724\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3013 - accuracy: 0.8724\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2984 - accuracy: 0.8765\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.8807\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8765\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8765\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2890 - accuracy: 0.8765\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8833\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7024 - accuracy: 0.5661\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6476 - accuracy: 0.6777\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.7273\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5739 - accuracy: 0.7562\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5464 - accuracy: 0.7686\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5230 - accuracy: 0.7893\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5012 - accuracy: 0.8017\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.8058\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.8223\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.8347\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8471\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8512\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8554\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3986 - accuracy: 0.8595\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3894 - accuracy: 0.8595\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8595\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8636\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3661 - accuracy: 0.8719\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8678\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8719\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3481 - accuracy: 0.8719\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8760\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8760\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3344 - accuracy: 0.8760\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8760\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8719\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.8719\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8719\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3133 - accuracy: 0.8719\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8719\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8719\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8802\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8760\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8760\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.8843\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8884\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2872 - accuracy: 0.8884\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2843 - accuracy: 0.8884\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.8926\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2785 - accuracy: 0.8967\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3505 - accuracy: 0.8361\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7382 - accuracy: 0.4298\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6800 - accuracy: 0.5744\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6653\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6028 - accuracy: 0.7149\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5725 - accuracy: 0.7479\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5471 - accuracy: 0.7769\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 0.7851\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.8017\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.8140\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4665 - accuracy: 0.8182\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.8223\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8223\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4250 - accuracy: 0.8182\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4142 - accuracy: 0.8182\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8264\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8347\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8347\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3781 - accuracy: 0.8347\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3705 - accuracy: 0.8388\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8471\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8512\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8554\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8595\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3401 - accuracy: 0.8636\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3353 - accuracy: 0.8636\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8719\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8719\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8719\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3161 - accuracy: 0.8719\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8719\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8760\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8760\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3011 - accuracy: 0.8760\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.8760\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8802\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2906 - accuracy: 0.8884\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.8884\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2837 - accuracy: 0.8884\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8884\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2776 - accuracy: 0.8884\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4101 - accuracy: 0.8361\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7532 - accuracy: 0.3967\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.7203 - accuracy: 0.4587\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6930 - accuracy: 0.5165\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.5455\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6540 - accuracy: 0.6240\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6366 - accuracy: 0.6777\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.7231\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6030 - accuracy: 0.7438\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.7975\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5685 - accuracy: 0.8099\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5489 - accuracy: 0.8182\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.8347\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8430\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.8388\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.8430\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8471\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4153 - accuracy: 0.8512\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8636\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3830 - accuracy: 0.8636\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3700 - accuracy: 0.8678\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8719\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8678\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8719\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8719\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8719\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8719\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8760\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8843\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8843\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.8843\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.3003 - accuracy: 0.8884\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8884\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8884\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2902 - accuracy: 0.8884\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.8926\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.9008\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.9008\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.9008\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.9008\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.9008\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.4049 - accuracy: 0.8197\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.5597\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6370 - accuracy: 0.6173\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6073 - accuracy: 0.6667\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.5854 - accuracy: 0.7325\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5666 - accuracy: 0.8066\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5510 - accuracy: 0.8066\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.8148\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5216 - accuracy: 0.8272\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5072 - accuracy: 0.8395\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4931 - accuracy: 0.8560\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4793 - accuracy: 0.8519\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4642 - accuracy: 0.8601\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.8642\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.8642\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8683\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8765\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8765\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3641 - accuracy: 0.8930\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.9053\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.9053\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3226 - accuracy: 0.9053\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.9095\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.9095\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2937 - accuracy: 0.9136\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2870 - accuracy: 0.9095\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.9095\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.9095\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2708 - accuracy: 0.9095\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2661 - accuracy: 0.9136\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2621 - accuracy: 0.9136\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2582 - accuracy: 0.9177\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9136\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2518 - accuracy: 0.9136\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.9136\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2458 - accuracy: 0.9095\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2432 - accuracy: 0.9095\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9136\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9136\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2356 - accuracy: 0.9136\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.2333 - accuracy: 0.9136\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6587 - accuracy: 0.6667\n",
            "Epoch 1/40\n",
            "8/8 [==============================] - 1s 3ms/step - loss: 0.8710 - accuracy: 0.4609\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7748 - accuracy: 0.5144\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5802\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6299 - accuracy: 0.6790\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5843 - accuracy: 0.7243\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.5488 - accuracy: 0.7490\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.5186 - accuracy: 0.7613\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7695\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.8025\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.8272\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.8395\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.8477\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8519\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8560\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8601\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8642\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8601\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8642\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8560\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8560\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8560\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3493 - accuracy: 0.8560\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3442 - accuracy: 0.8601\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3395 - accuracy: 0.8601\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8601\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8601\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.3269 - accuracy: 0.8601\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8601\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8601\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8601\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.8601\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8601\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.8601\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8642\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2981 - accuracy: 0.8642\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8642\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.8683\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2891 - accuracy: 0.8683\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.2858 - accuracy: 0.8683\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.2830 - accuracy: 0.8683\n",
            "2/2 [==============================] - 0s 4ms/step - loss: 0.2374 - accuracy: 0.9333\n",
            "Epoch 1/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.7454 - accuracy: 0.4917\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5611\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6428 - accuracy: 0.6172\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.6124 - accuracy: 0.6733\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.6964\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.7228\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5530 - accuracy: 0.7492\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7591\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.7855\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.7954\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.8152\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4625 - accuracy: 0.8185\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.8350\n",
            "Epoch 14/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8416\n",
            "Epoch 15/30\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.4090 - accuracy: 0.8449\n",
            "Epoch 16/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8548\n",
            "Epoch 17/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8548\n",
            "Epoch 18/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8614\n",
            "Epoch 19/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8614\n",
            "Epoch 20/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8647\n",
            "Epoch 21/30\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3378 - accuracy: 0.8812\n",
            "Epoch 22/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8779\n",
            "Epoch 23/30\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3244 - accuracy: 0.8878\n",
            "Epoch 24/30\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8878\n",
            "Epoch 25/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8878\n",
            "Epoch 26/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8878\n",
            "Epoch 27/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8911\n",
            "Epoch 28/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8944\n",
            "Epoch 29/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8911\n",
            "Epoch 30/30\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTTTvImlDP_K",
        "outputId": "e4c01b09-0642-4393-c869-89dff5dde259"
      },
      "source": [
        "# your grid_result object should be able to run in this code \n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.8315300583839417 using {'epochs': 30, 'units': 36}\n",
            "Means: 0.8183060169219971, Stdev: 0.04644036905807477 with: {'epochs': 30, 'units': 24}\n",
            "Means: 0.8315300583839417, Stdev: 0.05735455892564257 with: {'epochs': 30, 'units': 36}\n",
            "Means: 0.8281420707702637, Stdev: 0.06599681460300777 with: {'epochs': 40, 'units': 24}\n",
            "Means: 0.8183606624603271, Stdev: 0.08585851981890562 with: {'epochs': 40, 'units': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifYl2HCFDP_K"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}